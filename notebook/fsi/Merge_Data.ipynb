{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, install and import required libraries, watermark our file, initialise our Spark Session Builder and initialise our environment with required configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install watermark\n",
    "%pip install Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "import watermark\n",
    "from minio import Minio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Customer Churn ingest Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "'--packages \\\n",
    "org.postgresql:postgresql:42.2.10,\\\n",
    "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,\\\n",
    "org.apache.kafka:kafka-clients:2.4.0,\\\n",
    "org.apache.spark:spark-streaming_2.11:2.4.5,\\\n",
    "org.apache.hadoop:hadoop-aws:2.7.3 \\\n",
    "--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://172.30.141.91:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--master spark://' + os.environ['SPARK_CLUSTER'] + ':7077 pyspark-shell '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = sparkSessionBuilder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Declare our input data sources, import and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataFrame_Products = spark.read\\\n",
    "#                 .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "#                 .csv(\"s3a://rawdata/Customer-Churn_P2.csv\")\n",
    "# dataFrame_Products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from  pyspark.sql.functions import *\n",
    "\n",
    "srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "srcKakaTopic = \"data\"\n",
    "\n",
    "\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add(\"customerID\", IntegerType())\\\n",
    "    .add(\"Premium\", StringType())\\\n",
    "    .add(\"RelationshipManager\", StringType())\\\n",
    "    .add(\"PrimaryChannel\", StringType())\\\n",
    "    .add(\"HasCreditCard\", StringType())\\\n",
    "    .add(\"DebitCard\", StringType())\\\n",
    "    .add(\"IncomeProtection\", StringType())\\\n",
    "    .add(\"WealthManagement\", StringType())\\\n",
    "    .add(\"HomeEquityLoans\", StringType())\\\n",
    "    .add(\"MoneyMarketAccount\", StringType())\\\n",
    "    .add(\"CreditRating\", StringType())\\\n",
    "    .add(\"PaperlessBilling\", StringType())\\\n",
    "    .add(\"AccountType\", StringType())\\\n",
    "    .add(\"MonthlyCharges\", StringType())\\\n",
    "    .add(\"TotalCharges\", DoubleType())\\\n",
    "    .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "#Read from JSON Kafka messages into a dataframe\n",
    "dfKafka = spark.read.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "    .option(\"subscribe\", srcKakaTopic)\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\\\n",
    "    .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "    .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\")) \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "    .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "dfObj = spark.read.schema(schema).json(dfKafka)\n",
    "dfObj.printSchema()\n",
    "dfObj.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrom_All = dataFrame_Customer.join(dfObj, \"customerID\", how=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push prepared data to object storage and stop Spark cluster to save resources\n",
    "###  Note - be sure to change this user_id on the next line to your username (something in the range user1 ... user30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = \"user29\"\n",
    "file_location = \"s3a://data/full_data_csv\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit ('MacOS')",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
