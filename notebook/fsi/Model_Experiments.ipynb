{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage.\n",
    "\n",
    "### First, install and import required libraries and watermark our file - to show what libraries and versions we're using. Then define utility functions to integrate with our Object storage and _Verta_ visualisation server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "\n",
    "import watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "import verta.integrations.sklearn\n",
    "from minio import Minio\n",
    "from verta import Client\n",
    "from minio.error import ResponseError\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# import tools as tools\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In this next section, on the third line, change experiment_name by appending your username to _customerchurn_, e.g., if your username is user1: \n",
    "#### experiment_name = \"customerchurnuser1\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_name = \"customerchurnuser29\"\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "def get_verta():\n",
    "    client = Client(\"http://172.30.65.57:3000\")\n",
    "    return client\n",
    "\n",
    "def get_meta_store():\n",
    "    client = get_verta()\n",
    "    proj = client.set_project(\"ml-workshop\")\n",
    "    client.set_experiment(experiment_name)\n",
    "    run = client.set_experiment_run(experiment_id)\n",
    "    return run\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In this next section, on the second line, insert the value you retrieved from Minio object storage earlier - representing the fully qualified name of your csv file in Minio. This is the file pushed by the data engineer in the format: full_data_csv{USERNAME}/{FILENAME}.csv. \n",
    "#### In my case this value is: full_data_csvuser29/part-00000-59149e08-583c-46a5-bfa0-0b3abecbf1a3-c000.csv (yours will be different)\n",
    "### We refer to this fully qualified name in the Github instructions as CSV-FILE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "minioClient = get_s3_server()\n",
    "data_file = minioClient.fget_object(\"data\", \"full_data_csvuser29/part-00000-b6e27ca2-1458-4b44-ba29-b88aa9e90186-c000.csv\", \"/tmp/data.csv\")\n",
    "data_file_version = data_file.version_id\n",
    "data = pd.read_csv('/tmp/data.csv')\n",
    "data.head(5)\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use pandas.DataFrame functions\n",
    "- _shape_ to return the dimensionality\n",
    "- _info_ to print a concise summary of the DataFrame\n",
    "- _describe_ to generate descriptive statistics of the DataFrame's columns\n",
    "- _isnull().sum()_ to sum the empty values\n",
    "- finally determine Churn and Total Changes \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.shape"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.info()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.describe()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert binary variable into numeric so plotting is easier. We need to later take mean\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.replace(\" \", np.nan, inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.isna().sum()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mean = data['TotalCharges'].mean()\n",
    "data.fillna(mean, inplace=True)\n",
    "# Now we know that total charges has nan values\n",
    "data.isna().sum()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering pipeline\n",
    "### Use category_encoder's Ordinal encoding method which uses a single column of integers to represent the classes - then fit that to our 2 dimensional data imported earlier. Then pickle it and transform it. Then use Onehot (or dummy) coding for categorical features, producing one feature per category, each binary.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import category_encoders as ce\n",
    "import joblib\n",
    "\n",
    "names = ['gender', 'Partner', 'Dependents', 'Premium', 'HomeEquityLoans', 'MoneyMarketAccount', 'PaperlessBilling', 'Churn']\n",
    "# for column in names:\n",
    "#     labelencoder(column)\n",
    "\n",
    "enc = ce.ordinal.OrdinalEncoder(cols=names)\n",
    "enc.fit(data)\n",
    "joblib.dump(enc, 'enc.pkl')\n",
    "labelled_set = enc.transform(data)\n",
    "labelled_set.tail(5)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "names = ['RelationshipManager', 'PrimaryChannel', 'CreditRating', 'AccountType', 'HasCreditCard', 'DebitCard',\n",
    "         'IncomeProtection', 'WealthManagement']\n",
    "\n",
    "ohe = ce.OneHotEncoder(cols=names)\n",
    "ohe.fit(labelled_set)\n",
    "joblib.dump(ohe, 'ohe.pkl')\n",
    "final_set = ohe.transform(labelled_set)\n",
    "final_set.tail(5)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we use scikit-learn's 'train_test_split' function to randomly split our data into training and testing sets. Then remove the _Churn_ and _customerID_ fields from our training and testing datasets and output the shaope of our data."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = final_set['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_set, labels, test_size=0.2)\n",
    "X_train.pop('Churn')\n",
    "X_train.pop('customerID')\n",
    "X_test.pop('Churn')\n",
    "X_test.pop('customerID')\n",
    "print ('Training Data Shape',X_train.shape, y_train.shape)\n",
    "print ('Testing Data Shape',X_test.shape, y_test.shape)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Data For cross validation and GridSearch\n",
    "Y = final_set['Churn']\n",
    "X = final_set.drop(['Churn', 'customerID'], axis=1)\n",
    "print ('Training Data Shape', X.shape)\n",
    "print ('Testing Data Shape', Y.shape)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create DecisionTreeClassifier object, extract hyper parameters, and then GridSearch will best_model from the various inputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create decision tree object\n",
    "DT = DecisionTreeClassifier()\n",
    "# List of parameters\n",
    "# entropy\n",
    "criterion = ['gini']\n",
    "max_depth = [5,10,15]\n",
    "min_samples_split = [2,4,6]\n",
    "min_samples_leaf = [4,5,6,8]\n",
    "# Save all the lists in the variable\n",
    "hyperparameters = dict(max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = GridSearchCV(DT, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X,Y)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a DecisionTreeClassifier, assess results, print and store hyper parameters and accuracy and tag using 'DecisionTreeClassifier'\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = DecisionTreeClassifier(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'dct.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n",
    "store = get_meta_store()\n",
    "store.log_hyperparameters(hyperparameters)\n",
    "store.log_model(model)\n",
    "store.log_metric('Accuracy',results.mean()*100)\n",
    "store.log_tag(\"DecisionTreeClassifier\")\n",
    "# get_meta_store().log_dataset_version(\"raw_data\", dataset_version)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Like before, in this next section, on the third line, change experiment_name by appending your username to _customerchurn_, e.g., if your username is user1: \n",
    "#### experiment_name = \"customerchurnuser1\"\n",
    "### Create RandomForestClassifier object, extract hyper parameters, and then the best_model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_name = \"customerchurnuser29\"\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "\n",
    "# Create random forest object\n",
    "RF = RandomForestClassifier()\n",
    "n_estimators = [18,22]\n",
    "criterion = ['gini', 'entropy']\n",
    "# Create a list of all of the parameters\n",
    "max_depth = [30,40,50]\n",
    "min_samples_split = [6,8]\n",
    "min_samples_leaf = [8,10,12]\n",
    "# Merge the list into the variable\n",
    "hyperparameters = dict(n_estimators = n_estimators,max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(RF, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X, Y)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract best scores, params, criteria and depth from our model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best estimator:', best_model.best_estimator_.get_params()['n_estimators'])\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As above, use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a RandomForestClassifier, assess results, print and store hyper parameters and accuracy and tag using 'RandomForestClassifier'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = RandomForestClassifier(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'rft.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n",
    "store = get_meta_store()\n",
    "store.log_hyperparameters(hyperparameters)\n",
    "store.log_model(model)\n",
    "store.log_metric('Accuracy',results.mean()*100)\n",
    "store.log_tag(\"RandomForestClassifier\")\n",
    "store.log_attribute(\"data_file_location\", \"data/full_data_csv/a.csv\")\n",
    "store.log_attribute(\"data_file_version\", data_file_version)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Notebook complete')"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}